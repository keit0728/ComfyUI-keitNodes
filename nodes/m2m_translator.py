from typing import Any
import torch
from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
import langid
import os
import folder_paths

MODEL_CONFIGS = {
    "418M": {
        "model_name": "facebook/m2m100_418M",
        "cache_dir": "models--facebook--m2m100_418M",
    },
    "1.2B": {
        "model_name": "facebook/m2m100_1.2B",
        "cache_dir": "models--facebook--m2m100_1.2B",
    },
}


class M2MTranslator:
    """
    M2M-100å¤šè¨€èªç¿»è¨³
    """

    # ã‚¯ãƒ©ã‚¹å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒï¼ˆè¤‡æ•°ãƒãƒ¼ãƒ‰ã§å…±æœ‰ï¼‰
    _model: M2M100ForConditionalGeneration | None = None
    _tokenizer: Any | None = None
    _current_model_name: str | None = None

    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.base_cache_dir = os.path.join(folder_paths.models_dir, "keit-nodes")

    @classmethod
    def INPUT_TYPES(cls):
        # M2M-100ã§ã‚µãƒãƒ¼ãƒˆã™ã‚‹è¨€èªãƒªã‚¹ãƒˆï¼ˆä¸»è¦ãªã‚‚ã®ã‚’æŠœç²‹ï¼‰
        languages = [
            "auto_detect",
            "ja",  # æ—¥æœ¬èª
            "en",  # è‹±èª
            "zh",  # ä¸­å›½èª
            "ko",  # éŸ“å›½èª
            "fr",  # ãƒ•ãƒ©ãƒ³ã‚¹èª
            "de",  # ãƒ‰ã‚¤ãƒ„èª
            "es",  # ã‚¹ãƒšã‚¤ãƒ³èª
            "ru",  # ãƒ­ã‚·ã‚¢èª
            "ar",  # ã‚¢ãƒ©ãƒ“ã‚¢èª
            "hi",  # ãƒ’ãƒ³ãƒ‡ã‚£ãƒ¼èª
            "pt",  # ãƒãƒ«ãƒˆã‚¬ãƒ«èª
            "it",  # ã‚¤ã‚¿ãƒªã‚¢èª
            "tr",  # ãƒˆãƒ«ã‚³èª
            "th",  # ã‚¿ã‚¤èª
            "vi",  # ãƒ™ãƒˆãƒŠãƒ èª
            "pl",  # ãƒãƒ¼ãƒ©ãƒ³ãƒ‰èª
            "nl",  # ã‚ªãƒ©ãƒ³ãƒ€èª
            "sv",  # ã‚¹ã‚¦ã‚§ãƒ¼ãƒ‡ãƒ³èª
            "da",  # ãƒ‡ãƒ³ãƒãƒ¼ã‚¯èª
            "no",  # ãƒãƒ«ã‚¦ã‚§ãƒ¼èª
            "fi",  # ãƒ•ã‚£ãƒ³ãƒ©ãƒ³ãƒ‰èª
            "cs",  # ãƒã‚§ã‚³èª
            "hu",  # ãƒãƒ³ã‚¬ãƒªãƒ¼èª
            "ro",  # ãƒ«ãƒ¼ãƒãƒ‹ã‚¢èª
            "bg",  # ãƒ–ãƒ«ã‚¬ãƒªã‚¢èª
            "hr",  # ã‚¯ãƒ­ã‚¢ãƒã‚¢èª
            "sk",  # ã‚¹ãƒ­ãƒã‚­ã‚¢èª
            "sl",  # ã‚¹ãƒ­ãƒ™ãƒ‹ã‚¢èª
            "et",  # ã‚¨ã‚¹ãƒˆãƒ‹ã‚¢èª
            "lv",  # ãƒ©ãƒˆãƒ“ã‚¢èª
            "lt",  # ãƒªãƒˆã‚¢ãƒ‹ã‚¢èª
            "uk",  # ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠèª
            "be",  # ãƒ™ãƒ©ãƒ«ãƒ¼ã‚·èª
            "mk",  # ãƒã‚±ãƒ‰ãƒ‹ã‚¢èª
            "sq",  # ã‚¢ãƒ«ãƒãƒ‹ã‚¢èª
            "sr",  # ã‚»ãƒ«ãƒ“ã‚¢èª
            "bs",  # ãƒœã‚¹ãƒ‹ã‚¢èª
            "is",  # ã‚¢ã‚¤ã‚¹ãƒ©ãƒ³ãƒ‰èª
            "ga",  # ã‚¢ã‚¤ãƒ«ãƒ©ãƒ³ãƒ‰èª
            "cy",  # ã‚¦ã‚§ãƒ¼ãƒ«ã‚ºèª
            "ca",  # ã‚«ã‚¿ãƒ«ãƒ¼ãƒ‹ãƒ£èª
            "fa",  # ãƒšãƒ«ã‚·ã‚¢èª
            "ur",  # ã‚¦ãƒ«ãƒ‰ã‚¥ãƒ¼èª
            "bn",  # ãƒ™ãƒ³ã‚¬ãƒ«èª
            "ta",  # ã‚¿ãƒŸãƒ«èª
            "te",  # ãƒ†ãƒ«ã‚°èª
            "kn",  # ã‚«ãƒ³ãƒŠãƒ€èª
            "ml",  # ãƒãƒ©ãƒ¤ãƒ¼ãƒ©ãƒ èª
            "gu",  # ã‚°ã‚¸ãƒ£ãƒ©ãƒ¼ãƒˆèª
            "pa",  # ãƒ‘ãƒ³ã‚¸ãƒ£ãƒ¼ãƒ–èª
            "mr",  # ãƒãƒ©ãƒ¼ãƒ†ã‚£ãƒ¼èª
            "ne",  # ãƒãƒ‘ãƒ¼ãƒ«èª
            "si",  # ã‚·ãƒ³ãƒãƒ©èª
            "my",  # ãƒ“ãƒ«ãƒèª
            "km",  # ã‚¯ãƒ¡ãƒ¼ãƒ«èª
            "lo",  # ãƒ©ã‚ªèª
            "ka",  # ã‚¸ãƒ§ãƒ¼ã‚¸ã‚¢èª
            "hy",  # ã‚¢ãƒ«ãƒ¡ãƒ‹ã‚¢èª
            "az",  # ã‚¢ã‚¼ãƒ«ãƒã‚¤ã‚¸ãƒ£ãƒ³èª
            "kk",  # ã‚«ã‚¶ãƒ•èª
            "uz",  # ã‚¦ã‚ºãƒ™ã‚¯èª
            "mn",  # ãƒ¢ãƒ³ã‚´ãƒ«èª
            "he",  # ãƒ˜ãƒ–ãƒ©ã‚¤èª
            "yi",  # ã‚¤ãƒ‡ã‚£ãƒƒã‚·ãƒ¥èª
            "sw",  # ã‚¹ãƒ¯ãƒ’ãƒªèª
            "zu",  # ã‚ºãƒ¼ãƒ«ãƒ¼èª
            "xh",  # ã‚³ã‚µèª
            "af",  # ã‚¢ãƒ•ãƒªã‚«ãƒ¼ãƒ³ã‚¹èª
            "am",  # ã‚¢ãƒ ãƒãƒ©èª
            "ha",  # ãƒã‚¦ã‚µèª
            "ig",  # ã‚¤ãƒœèª
            "yo",  # ãƒ¨ãƒ«ãƒèª
            "so",  # ã‚½ãƒãƒªèª
            "mg",  # ãƒãƒ€ã‚¬ã‚¹ã‚«ãƒ«èª
            "id",  # ã‚¤ãƒ³ãƒ‰ãƒã‚·ã‚¢èª
            "ms",  # ãƒãƒ¬ãƒ¼èª
            "tl",  # ã‚¿ã‚¬ãƒ­ã‚°èª
            "jv",  # ã‚¸ãƒ£ãƒ¯èª
            "su",  # ã‚¹ãƒ³ãƒ€èª
            "ceb",  # ã‚»ãƒ–ã‚¢ãƒèª
        ]

        return {
            "required": {
                "text": ("STRING", {"multiline": True, "default": "ã“ã‚“ã«ã¡ã¯ã€ä¸–ç•Œ"}),
                "source_language": (languages, {"default": "auto_detect"}),
                "target_language": (
                    languages[1:],
                    {"default": "en"},  # auto_detectã‚’é™¤å¤–
                ),
                "model_size": (["418M", "1.2B"], {"default": "418M"}),
            },
            "optional": {
                "num_beams": ("INT", {"default": 5, "min": 1, "max": 10, "step": 1}),
            },
        }

    RETURN_TYPES = ("STRING", "STRING", "FLOAT")
    RETURN_NAMES = ("translated_text", "detected_language", "confidence")

    FUNCTION = "translate"
    CATEGORY = "ğŸŒ Translation/M2M-100"

    def load_model(self, model_size) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰"""
        model_name = MODEL_CONFIGS[model_size]["model_name"]
        cache_path = os.path.join(
            self.base_cache_dir, MODEL_CONFIGS[model_size]["cache_dir"]
        )

        # ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä½•ã‚‚ã—ãªã„
        if self._model is not None and self._current_model_name == model_name:
            return

        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å‡¦ç†
        print(f"Loading M2M-100 {model_size} model...")

        if torch.cuda.is_available():
            self._model = M2M100ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                cache_dir=cache_path,
            ).cuda()
        else:
            self._model = M2M100ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                cache_dir=cache_path,
            )

        self._tokenizer = M2M100Tokenizer.from_pretrained(
            model_name,
            cache_dir=cache_path,
        )
        self._current_model_name = model_name
        print("Model loaded successfully!")

    def detect_language(self, text) -> tuple[str, float]:
        """è¨€èªã‚’è‡ªå‹•æ¤œå‡º"""
        lang_code, confidence = langid.classify(text)
        return lang_code, confidence

    def translate(
        self,
        text,
        source_language,
        target_language,
        model_size,
        num_beams=5,
    ):
        """ç¿»è¨³"""
        # åŒã˜è¨€èªã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if source_language == target_language:
            return (text, source_language, confidence)

        # è¨€èªè‡ªå‹•æ¤œå‡º
        if source_language == "auto_detect":
            source_language, confidence = self.detect_language(text)
            print(
                f"Detected language: {source_language} (confidence: {confidence:.2f})"
            )
        else:
            confidence = 1.0

        # ç¿»è¨³å®Ÿè¡Œ
        self.load_model(model_size)
        self._tokenizer.src_lang = source_language
        inputs = self._tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(self.device)
        with torch.no_grad():
            generated_tokens = self._model.generate(
                **inputs,
                forced_bos_token_id=self._tokenizer.get_lang_id(target_language),
                num_beams=num_beams,
                early_stopping=True,
                use_cache=True,
            )
        translated_texts = self._tokenizer.batch_decode(
            generated_tokens, skip_special_tokens=True
        )[0]

        return (translated_texts, source_language, float(confidence))
